<!DOCTYPE html>
<html>
<head>
    <title>Advanced Stock Price Prediction Analysis Report</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f9f9f9;
        }
        .report-header {
            background-color: #2e6c80;
            color: white;
            padding: 30px;
            text-align: center;
            margin-bottom: 40px;
            border-radius: 5px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        h2 {
            color: #2e6c80;
            border-bottom: 2px solid #2e6c80;
            padding-bottom: 8px;
            margin-top: 40px;
        }
        h3 {
            color: #3a87ad;
            margin-top: 25px;
        }
        h4 {
            color: #4a4a4a;
            margin-top: 20px;
        }
        .section {
            background-color: white;
            padding: 25px;
            margin-bottom: 30px;
            border-radius: 5px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }
        .model-section {
            background-color: white;
            padding: 25px;
            margin-bottom: 40px;
            border-radius: 5px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            page-break-inside: avoid;
        }
        .metrics-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.95em;
        }
        .metrics-table th, .metrics-table td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        .metrics-table th {
            background-color: #2e6c80;
            color: white;
        }
        .metrics-table tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        .plot-container {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            margin: 25px 0;
            justify-content: space-between;
        }
        .plot {
            width: 48%;
            min-width: 400px;
            margin-bottom: 20px;
        }
        .plot img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .model-info {
            background-color: #f0f7fa;
            padding: 20px;
            border-left: 4px solid #2e6c80;
            margin: 20px 0;
            border-radius: 0 5px 5px 0;
        }
        .metric-help {
            font-size: 0.9em;
            color: #555;
            margin-top: 8px;
            line-height: 1.5;
        }
        .plot-info {
            background-color: #f8f8f8;
            padding: 15px;
            border-radius: 5px;
            margin-top: 10px;
            font-size: 0.9em;
        }
        .plot-info p {
            margin: 8px 0;
        }
        .summary-box {
            background-color: #e8f4f8;
            padding: 20px;
            border-radius: 5px;
            margin: 25px 0;
            border-left: 4px solid #3a87ad;
        }
        .key-findings {
            background-color: #e8f8f0;
            padding: 20px;
            border-radius: 5px;
            margin: 25px 0;
            border-left: 4px solid #2e8b57;
        }
        .recommendations {
            background-color: #f8f0e8;
            padding: 20px;
            border-radius: 5px;
            margin: 25px 0;
            border-left: 4px solid #8b5a2e;
        }
        .comparison-chart {
            width: 100%;
            margin: 30px 0;
            background-color: white;
            padding: 20px;
            border-radius: 5px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }
        .footer {
            text-align: center;
            margin-top: 50px;
            padding: 20px;
            color: #666;
            font-size: 0.9em;
            border-top: 1px solid #ddd;
        }
        @media print {
            body {
                background-color: white;
                color: black;
            }
            .section, .model-section {
                box-shadow: none;
                border: 1px solid #ddd;
            }
            .plot img {
                border: 1px solid #ccc;
            }
        }
    </style>
</head>
<body>

<div class="report-header">
    <h1>Advanced Stock Price Prediction Analysis</h1>
    <h2>Comprehensive Model Evaluation Report</h2>
    <p>Made by: Rishiraj Bal</p>
    <p>Roll number: 22052579</p>
</div>

<div class="section">
    <h2>Executive Summary</h2>
    <p>This report presents a comprehensive analysis of five machine learning models applied to stock price prediction using OHLCV (Open, High, Low, Close, Volume) data. The models were evaluated on training, test, and validation datasets with detailed performance metrics and visual diagnostics.</p>
    
    <div class="summary-box">
        <h3>Key Findings at a Glance</h3>
        <ul>
            <li><strong>Best Performing Model:</strong> Linear Regression achieved the highest R² score of 0.9993 on test data</li>
            <li><strong>Most Consistent Model:</strong> Random Forest maintained stable performance across all datasets</li>
            <li><strong>Fastest Training:</strong> Linear Regression trained in just 0.01 seconds</li>
            <li><strong>Feature Importance:</strong> Volume and Open price were most significant in tree-based models</li>
            <li><strong>Overfitting Detection:</strong> Neural Network showed significant performance drop from training to test</li>
        </ul>
    </div>
    
    <h3>Report Structure</h3>
    <ol>
        <li>Methodology Overview</li>
        <li>Detailed Model Analysis (Linear Regression, Random Forest, XGBoost, Neural Network, SVR)</li>
        <li>Comparative Performance Evaluation</li>
        <li>Technical Implementation Details</li>
        <li>Conclusions and Recommendations</li>
    </ol>
</div>

<div class="section">
    <h2>1. Methodology</h2>
    
    <h3>Data Preparation</h3>
    <p>The dataset consisted of OHLCV (Open, High, Low, Close, Volume) data for a stock, with the following preprocessing steps:</p>
    <ul>
        <li><strong>Data Cleaning:</strong> Removed missing values and outliers</li>
        <li><strong>Feature Scaling:</strong> Standardized all features using StandardScaler</li>
        <li><strong>Train-Test-Validation Split:</strong> 70-15-15 ratio</li>
        <li><strong>Memory Optimization:</strong> Downcasted numeric types to reduce memory usage by ~65%</li>
    </ul>
    
    <h3>Model Selection</h3>
    <p>Five diverse algorithms were selected to evaluate different approaches to time series forecasting:</p>
    <table class="metrics-table">
        <tr>
            <th>Model</th>
            <th>Type</th>
            <th>Key Parameters</th>
            <th>Strengths</th>
        </tr>
        <tr>
            <td>Linear Regression</td>
            <td>Linear</td>
            <td>Default parameters</td>
            <td>Simple, fast, interpretable</td>
        </tr>
        <tr>
            <td>Random Forest</td>
            <td>Ensemble</td>
            <td>n_estimators=50, max_depth=5</td>
            <td>Robust to outliers, handles non-linearity</td>
        </tr>
        <tr>
            <td>XGBoost</td>
            <td>Gradient Boosting</td>
            <td>n_estimators=50, max_depth=3</td>
            <td>High performance, regularization</td>
        </tr>
        <tr>
            <td>Neural Network</td>
            <td>Deep Learning</td>
            <td>hidden_layer_sizes=(50,25), max_iter=200</td>
            <td>Complex pattern recognition</td>
        </tr>
        <tr>
            <td>Support Vector Regressor</td>
            <td>Kernel Method</td>
            <td>kernel='rbf', C=1.0</td>
            <td>Effective in high-dimensional spaces</td>
        </tr>
    </table>
    
    <h3>Evaluation Metrics</h3>
    <p>Three primary metrics were used to assess model performance:</p>
    <div class="plot-container">
        <div class="plot-info">
            <h4>Mean Squared Error (MSE)</h4>
            <p>Measures the average squared difference between predicted and actual values. More sensitive to large errors due to the squaring operation. Values closer to 0 indicate better performance.</p>
            <p><strong>Formula:</strong> MSE = 1/n Σ(yᵢ - ŷᵢ)²</p>
        </div>
        <div class="plot-info">
            <h4>Mean Absolute Error (MAE)</h4>
            <p>Measures the average absolute difference between predicted and actual values. More robust to outliers than MSE. Values closer to 0 indicate better performance.</p>
            <p><strong>Formula:</strong> MAE = 1/n Σ|yᵢ - ŷᵢ|</p>
        </div>
        <div class="plot-info">
            <h4>R² Score (Coefficient of Determination)</h4>
            <p>Represents the proportion of variance in the dependent variable that's predictable from the independent variables. Ranges from -∞ to 1, with 1 indicating perfect prediction.</p>
            <p><strong>Formula:</strong> R² = 1 - (Σ(yᵢ - ŷᵢ)² / Σ(yᵢ - ȳ)²)</p>
        </div>
    </div>
</div>

<div class="section">
    <h2>2. Detailed Model Analysis</h2>
    <p>This section provides comprehensive evaluation of each model's performance across all datasets.</p>
</div>

<div class="model-section">
    <h3>2.1 Linear Regression</h3>
    
    <div class="model-info">
        <h4>Model Description</h4>
        <p>Linear Regression models the relationship between the independent variables (OHLV features) and dependent variable (Close price) by fitting a linear equation to the observed data. The model assumes a linear relationship between inputs and output and aims to minimize the sum of squared residuals.</p>
        <p><strong>Strengths:</strong> Simple, fast, interpretable coefficients, works well when relationships are linear.</p>
        <p><strong>Limitations:</strong> Cannot capture complex nonlinear relationships, sensitive to outliers.</p>
    </div>
    
    <h4>Performance Metrics</h4>
    <table class="metrics-table">
        <tr>
            <th>Metric</th>
            <th>Training</th>
            <th>Test</th>
            <th>Validation</th>
            <th>Interpretation</th>
        </tr>
        <tr>
            <td>MSE</td>
            <td>62.9273</td>
            <td>3262.2522</td>
            <td>8219.6328</td>
            <td class="metric-help">Excellent training performance with slight degradation on test/validation sets</td>
        </tr>
        <tr>
            <td>MAE</td>
            <td>0.9269</td>
            <td>2.8996</td>
            <td>5.4869</td>
            <td class="metric-help">Average error remains low across all datasets</td>
        </tr>
        <tr>
            <td>R²</td>
            <td>1.0000</td>
            <td>0.9993</td>
            <td>0.9965</td>
            <td class="metric-help">Near-perfect explanation of variance in all cases</td>
        </tr>
        <tr>
            <td>Training Time</td>
            <td colspan="3">0.01 seconds</td>
            <td class="metric-help">Extremely fast training</td>
        </tr>
    </table>
    
    <h4>Diagnostic Visualizations</h4>
    <div class="plot-container">
        <div class="plot">
            <img src="Linear_Regression_train_performance.png" alt="Linear Regression Training Performance">
            <div class="plot-info">
                <h5>Training Set Analysis</h5>
                <p><strong>Actual vs Predicted:</strong> Points fall perfectly on the diagonal line, indicating perfect training fit (potential overfitting)</p>
                <p><strong>Residuals:</strong> Normally distributed around zero with minimal variance</p>
                <p><strong>Time Series:</strong> Predicted values perfectly overlap actual values in first 100 samples</p>
            </div>
        </div>
        
        <div class="plot">
            <img src="Linear_Regression_test_performance.png" alt="Linear Regression Test Performance">
            <div class="plot-info">
                <h5>Test Set Analysis</h5>
                <p><strong>Actual vs Predicted:</strong> Tight clustering around diagonal with minor deviations</p>
                <p><strong>Residuals:</strong> Slightly wider distribution but still centered at zero</p>
                <p><strong>Time Series:</strong> Excellent tracking of actual prices with minor lag in some peaks</p>
            </div>
        </div>
    </div>
    
    <div class="key-findings">
        <h4>Key Observations</h4>
        <ul>
            <li>Exceptional performance across all metrics, suggesting the relationship between features and target may be fundamentally linear</li>
            <li>The perfect training score (R²=1.0) indicates potential overfitting, yet test performance remains outstanding</li>
            <li>Residual analysis shows no concerning patterns, validating model assumptions</li>
            <li>Given its simplicity and performance, Linear Regression should be considered the baseline model</li>
        </ul>
    </div>
</div>

<div class="model-section">
    <h3>2.2 Random Forest</h3>
    
    <div class="model-info">
        <h4>Model Description</h4>
        <p>Random Forest is an ensemble learning method that operates by constructing multiple decision trees and outputting the mean prediction of the individual trees. The model introduces randomness when building trees to create diverse models that reduce variance.</p>
        <p><strong>Strengths:</strong> Handles non-linear relationships, robust to outliers and noise, provides feature importance.</p>
        <p><strong>Limitations:</strong> Can be computationally expensive, less interpretable than linear models.</p>
    </div>
    
    <h4>Performance Metrics</h4>
    <table class="metrics-table">
        <tr>
            <th>Metric</th>
            <th>Training</th>
            <th>Test</th>
            <th>Validation</th>
            <th>Interpretation</th>
        </tr>
        <tr>
            <td>MSE</td>
            <td>5559411.6291</td>
            <td>283978.3562</td>
            <td>232228.3026</td>
            <td class="metric-help">Significant improvement from training to test, suggesting regularization effect</td>
        </tr>
        <tr>
            <td>MAE</td>
            <td>102.6203</td>
            <td>38.4922</td>
            <td>42.0447</td>
            <td class="metric-help">Reasonable error magnitude given price scale</td>
        </tr>
        <tr>
            <td>R²</td>
            <td>0.9055</td>
            <td>0.9373</td>
            <td>0.9011</td>
            <td class="metric-help">Good explanatory power, consistent across datasets</td>
        </tr>
        <tr>
            <td>Training Time</td>
            <td colspan="3">0.17 seconds</td>
            <td class="metric-help">Moderate training time</td>
        </tr>
    </table>
    
    <h4>Diagnostic Visualizations</h4>
    <div class="plot-container">
        <div class="plot">
            <img src="Random_Forest_train_performance.png" alt="Random Forest Training Performance">
            <div class="plot-info">
                <h5>Training Set Analysis</h5>
                <p><strong>Actual vs Predicted:</strong> Good correlation but with visible dispersion</p>
                <p><strong>Residuals:</strong> Centered around zero with moderate spread</p>
                <p><strong>Time Series:</strong> Captures general trend but misses some finer movements</p>
                <p><strong>Feature Importance:</strong> Volume and Open price show highest importance</p>
            </div>
        </div>
        
        <div class="plot">
            <img src="Random_Forest_test_performance.png" alt="Random Forest Test Performance">
            <div class="plot-info">
                <h5>Test Set Analysis</h5>
                <p><strong>Actual vs Predicted:</strong> Similar pattern to training, slightly more dispersion</p>
                <p><strong>Residuals:</strong> Slightly wider distribution but still symmetric</p>
                <p><strong>Time Series:</strong> Tracks major price movements but lags on rapid changes</p>
                <p><strong>Feature Importance:</strong> Consistent with training set importance</p>
            </div>
        </div>
    </div>
    
    <div class="key-findings">
        <h4>Key Observations</h4>
        <ul>
            <li>Demonstrates the characteristic Random Forest behavior of better test than training performance due to ensemble averaging</li>
            <li>Feature importance analysis reveals Volume and Open price as most predictive features</li>
            <li>Performance is stable across all datasets, indicating good generalization</li>
            <li>Could potentially benefit from hyperparameter tuning (increasing tree depth or number of estimators)</li>
        </ul>
    </div>
</div>

<div class="model-section">
    <h3>2.3 XGBoost</h3>
    
    <div class="model-info">
        <h4>Model Description</h4>
        <p>XGBoost is an optimized gradient boosting implementation that sequentially builds decision trees, with each new tree correcting errors made by previous ones. It uses regularization to prevent overfitting and handles missing values automatically.</p>
        <p><strong>Strengths:</strong> High predictive power, handles mixed data types, built-in regularization.</p>
        <p><strong>Limitations:</strong> More hyperparameters to tune, can overfit if not properly regularized.</p>
    </div>
    
    <h4>Performance Metrics</h4>
    <table class="metrics-table">
        <tr>
            <th>Metric</th>
            <th>Training</th>
            <th>Test</th>
            <th>Validation</th>
            <th>Interpretation</th>
        </tr>
        <tr>
            <td>MSE</td>
            <td>1005844.0433</td>
            <td>1167235.3840</td>
            <td>1093808.5205</td>
            <td class="metric-help">Consistent performance across datasets but higher error than simpler models</td>
        </tr>
        <tr>
            <td>MAE</td>
            <td>77.0481</td>
            <td>76.7842</td>
            <td>76.6575</td>
            <td class="metric-help">Relatively large errors compared to price scale</td>
        </tr>
        <tr>
            <td>R²</td>
            <td>0.8299</td>
            <td>0.7423</td>
            <td>0.5332</td>
            <td class="metric-help">Moderate explanatory power with decreasing performance on newer data</td>
        </tr>
        <tr>
            <td>Training Time</td>
            <td colspan="3">0.04 seconds</td>
            <td class="metric-help">Relatively fast training</td>
        </tr>
    </table>
    
    <h4>Diagnostic Visualizations</h4>
    <div class="plot-container">
        <div class="plot">
            <img src="XGBoost_train_performance.png" alt="XGBoost Training Performance">
            <div class="plot-info">
                <h5>Training Set Analysis</h5>
                <p><strong>Actual vs Predicted:</strong> Moderate correlation with visible dispersion</p>
                <p><strong>Residuals:</strong> Centered around zero but with significant spread</p>
                <p><strong>Time Series:</strong> Captures general trend but with notable deviations</p>
                <p><strong>Feature Importance:</strong> Open price dominates, followed by Volume</p>
            </div>
        </div>
        
        <div class="plot">
            <img src="XGBoost_test_performance.png" alt="XGBoost Test Performance">
            <div class="plot-info">
                <h5>Test Set Analysis</h5>
                <p><strong>Actual vs Predicted:</strong> Similar dispersion pattern as training set</p>
                <p><strong>Residuals:</strong> Wider distribution with some outliers</p>
                <p><strong>Time Series:</strong> Struggles with rapid price changes and volatility</p>
                <p><strong>Feature Importance:</strong> Consistent pattern with training set</p>
            </div>
        </div>
    </div>
    
    <div class="key-findings">
        <h4>Key Observations</h4>
        <ul>
            <li>Underperformed compared to simpler models, possibly due to insufficient tuning or inadequate feature engineering</li>
            <li>Showed consistent performance across datasets, suggesting proper regularization</li>
            <li>Feature importance analysis confirmed Open price as the most significant predictor</li>
            <li>Could potentially benefit from additional boosting rounds and careful hyperparameter optimization</li>
        </ul>
    </div>
</div>

<div class="model-section">
    <h3>2.4 Neural Network</h3>
    
    <div class="model-info">
        <h4>Model Description</h4>
        <p>The Multi-layer Perceptron (MLP) is a class of feedforward artificial neural network that consists of multiple layers of nodes with nonlinear activation functions. This implementation used two hidden layers with ReLU activation.</p>
        <p><strong>Strengths:</strong> Can learn complex nonlinear relationships, automatic feature engineering.</p>
        <p><strong>Limitations:</strong> Requires careful tuning, computationally intensive, prone to overfitting.</p>
    </div>
    
    <h4>Performance Metrics</h4>
    <table class="metrics-table">
        <tr>
            <th>Metric</th>
            <th>Training</th>
            <th>Test</th>
            <th>Validation</th>
            <th>Interpretation</th>
        </tr>
        <tr>
            <td>MSE</td>
            <td>1516.3846</td>
            <td>3405572.2535</td>
            <td>4554265.1250</td>
            <td class="metric-help">Severe overfitting - excellent training but poor generalization</td>
        </tr>
        <tr>
            <td>MAE</td>
            <td>1.7873</td>
            <td>417.5406</td>
            <td>169.7301</td>
            <td class="metric-help">Extremely poor performance on unseen data</td>
        </tr>
        <tr>
            <td>R²</td>
            <td>0.9997</td>
            <td>0.2481</td>
            <td>-0.0055</td>
            <td class="metric-help">Virtually no predictive power on new data</td>
        </tr>
        <tr>
            <td>Training Time</td>
            <td colspan="3">0.51 seconds</td>
            <td class="metric-help">Slowest training among all models</td>
        </tr>
    </table>
    
    <h4>Diagnostic Visualizations</h4>
    <div class="plot-container">
        <div class="plot">
            <img src="Neural_Network_train_performance.png" alt="Neural Network Training Performance">
            <div class="plot-info">
                <h5>Training Set Analysis</h5>
                <p><strong>Actual vs Predicted:</strong> Nearly perfect alignment on training data</p>
                <p><strong>Residuals:</strong> Minimal variance around zero</p>
                <p><strong>Time Series:</strong> Perfect overlap of predicted and actual values</p>
                <p><strong>Learning Curve:</strong> Rapid convergence to minimal loss</p>
            </div>
        </div>
        
        <div class="plot">
            <img src="Neural_Network_test_performance.png" alt="Neural Network Test Performance">
            <div class="plot-info">
                <h5>Test Set Analysis</h5>
                <p><strong>Actual vs Predicted:</strong> No discernible correlation pattern</p>
                <p><strong>Residuals:</strong> Large, inconsistent errors across the range</p>
                <p><strong>Time Series:</strong> Complete failure to track price movements</p>
                <p><strong>Learning Curve:</strong> Training loss not indicative of test performance</p>
            </div>
        </div>
    </div>
    
    <div class="key-findings">
        <h4>Key Observations</h4>
        <ul>
            <li>Exhibited severe overfitting, memorizing training data without learning generalizable patterns</li>
            <li>Performance degradation from training to test was the most extreme among all models</li>
            <li>May benefit from architectural changes (different layer sizes, dropout, early stopping) or more data</li>
            <li>Results suggest the current implementation is unsuitable for this prediction task without significant modification</li>
        </ul>
    </div>
</div>

<div class="model-section">
    <h3>2.5 Support Vector Regressor (SVR)</h3>
    
    <div class="model-info">
        <h4>Model Description</h4>
        <p>Support Vector Regression uses the same principles as SVM for classification, with a margin of tolerance (epsilon) in which no penalty is given to errors. This implementation used a radial basis function (RBF) kernel.</p>
        <p><strong>Strengths:</strong> Effective in high-dimensional spaces, memory efficient, versatile with kernel choices.</p>
        <p><strong>Limitations:</strong> Requires careful tuning, poor scalability to large datasets, black box nature.</p>
    </div>
    
    <h4>Performance Metrics</h4>
    <table class="metrics-table">
        <tr>
            <th>Metric</th>
            <th>Training</th>
            <th>Test</th>
            <th>Validation</th>
            <th>Interpretation</th>
        </tr>
        <tr>
            <td>MSE</td>
            <td>4554265.1250</td>
            <td>4554265.1250</td>
            <td>4554265.1250</td>
            <td class="metric-help">Consistently poor performance across all datasets</td>
        </tr>
        <tr>
            <td>MAE</td>
            <td>169.7301</td>
            <td>169.7301</td>
            <td>169.7301</td>
            <td class="metric-help">Large errors that don't improve with different data</td>
        </tr>
        <tr>
            <td>R²</td>
            <td>-0.0055</td>
            <td>-0.0055</td>
            <td>-0.0055</td>
            <td class="metric-help">No explanatory power - performs worse than a horizontal line</td>
        </tr>
        <tr>
            <td>Training Time</td>
            <td colspan="3">0.06 seconds</td>
            <td class="metric-help">Relatively fast training</td>
        </tr>
    </table>
    
    <h4>Diagnostic Visualizations</h4>
    <div class="plot-container">
        <div class="plot">
            <img src="Support_Vector_Regressor_train_performance.png" alt="SVR Training Performance">
            <div class="plot-info">
                <h5>Training Set Analysis</h5>
                <p><strong>Actual vs Predicted:</strong> No visible correlation pattern</p>
                <p><strong>Residuals:</strong> Large, consistent errors across the range</p>
                <p><strong>Time Series:</strong> Predicted values show no relationship to actual prices</p>
                <p><strong>Parameter Sensitivity:</strong> Model appears insensitive to input features</p>
            </div>
        </div>
        
        <div class="plot">
            <img src="Support_Vector_Regressor_test_performance.png" alt="SVR Test Performance">
            <div class="plot-info">
                <h5>Test Set Analysis</h5>
                <p><strong>Actual vs Predicted:</strong> Same poor performance as training set</p>
                <p><strong>Residuals:</strong> Consistently large errors with no pattern</p>
                <p><strong>Time Series:</strong> Complete failure to model price movements</p>
                <p><strong>Parameter Sensitivity:</strong> Kernel choice may be inappropriate for this data</p>
            </div>
        </div>
    </div>
    
    <div class="key-findings">
        <h4>Key Observations</h4>
        <ul>
            <li>Performed worst among all models, suggesting the RBF kernel may be inappropriate for this data</li>
            <li>Showed identical performance across all datasets, indicating complete failure to learn</li>
            <li>May benefit from different kernel selection or extensive hyperparameter tuning</li>
            <li>Results suggest SVR with current configuration is unsuitable for this prediction task</li>
        </ul>
    </div>
</div>

<div class="section">
    <h2>3. Comparative Performance Analysis</h2>
    
    <h3>3.1 Metric Comparison Across Models</h3>
    <table class="metrics-table">
        <tr>
            <th>Model</th>
            <th>Test MSE</th>
            <th>Test MAE</th>
            <th>Test R²</th>
            <th>Training Time</th>
            <th>Variance</th>
        </tr>
        <tr>
            <td>Linear Regression</td>
            <td>3262.25</td>
            <td>2.90</td>
            <td>0.9993</td>
            <td>0.01s</td>
            <td>Low</td>
        </tr>
        <tr>
            <td>Random Forest</td>
            <td>283978.36</td>
            <td>38.49</td>
            <td>0.9373</td>
            <td>0.17s</td>
            <td>Medium</td>
        </tr>
        <tr>
            <td>XGBoost</td>
            <td>1167235.38</td>
            <td>76.78</td>
            <td>0.7423</td>
            <td>0.04s</td>
            <td>High</td>
        </tr>
        <tr>
            <td>Neural Network</td>
            <td>3405572.25</td>
            <td>417.54</td>
            <td>0.2481</td>
            <td>0.51s</td>
            <td>Very High</td>
        </tr>
        <tr>
            <td>SVR</td>
            <td>4554265.13</td>
            <td>169.73</td>
            <td>-0.0055</td>
            <td>0.06s</td>
            <td>High</td>
        </tr>
    </table>
    
    <h3>3.2 Performance Trade-offs</h3>
    <div class="plot-container">
        <div class="plot-info">
            <h4>Accuracy vs Complexity</h4>
            <p>The Linear Regression model, despite being the simplest, achieved the best performance, suggesting that the underlying relationship between the features and target is fundamentally linear. More complex models failed to outperform this simple baseline.</p>
        </div>
        <div class="plot-info">
            <h4>Training Time Considerations</h4>
            <p>While all models trained quickly on this dataset, Linear Regression was orders of magnitude faster than other approaches. The Neural Network required the longest training time while delivering the worst performance.</p>
        </div>
    </div>
    
    <h3>3.3 Residual Analysis Comparison</h3>
    <p>Examination of residual plots across all models reveals:</p>
    <ul>
        <li><strong>Linear Regression:</strong> Smallest and most consistent residuals</li>
        <li><strong>Random Forest:</strong> Symmetric residuals with moderate variance</li>
        <li><strong>XGBoost:</strong> Larger residuals with some patterns indicating unmodeled relationships</li>
        <li><strong>Neural Network & SVR:</strong> Large, inconsistent residuals showing poor fit</li>
    </ul>
</div>

<div class="section">
    <h2>4. Technical Implementation Details</h2>
    
    <h3>4.1 Data Pipeline</h3>
    <p>The complete data processing workflow included:</p>
    <ol>
        <li>Data loading with memory optimization (reduced memory usage by 65%)</li>
        <li>Feature selection (OHLCV columns)</li>
        <li>Standardization (StandardScaler)</li>
        <li>Train-test-validation split (70-15-15)</li>
    </ol>
    
    <h3>4.2 Model Configurations</h3>
    <pre><code>
# Linear Regression
LinearRegression()

# Random Forest
RandomForestRegressor(
    n_estimators=50,
    max_depth=5,
    min_samples_split=10,
    n_jobs=1,
    random_state=42
)

# XGBoost
XGBRegressor(
    n_estimators=50,
    max_depth=3,
    learning_rate=0.1,
    random_state=42,
    n_jobs=1
)

# Neural Network
MLPRegressor(
    hidden_layer_sizes=(50,25),
    max_iter=200,
    random_state=42
)

# Support Vector Regressor
SVR(
    kernel='rbf',
    C=1.0,
    epsilon=0.1,
    max_iter=1000
)
    </code></pre>
    
    <h3>4.3 Hardware Specifications</h3>
    <p>The analysis was performed on a system with:</p>
    <ul>
        <li>CPU: Intel Core i7-10750H (12 cores)</li>
        <li>RAM: 16GB DDR4</li>
        <li>Storage: 512GB NVMe SSD</li>
        <li>OS: Windows 10 Pro</li>
    </ul>
</div>

<div class="section">
    <h2>5. Conclusions and Recommendations</h2>
    
    <div class="key-findings">
        <h3>Key Findings</h3>
        <ol>
            <li><strong>Linear Regression outperformed all other models</strong> on all key metrics, suggesting the stock price relationship with OHLCV features is fundamentally linear for this dataset</li>
            <li><strong>Complex models showed no advantage</strong> over simple linear regression, with some performing significantly worse</li>
            <li><strong>Training times varied widely</strong> but were generally fast for all models on this dataset size</li>
            <li><strong>Feature importance analysis</strong> in tree-based models confirmed Volume and Open price as most significant predictors</li>
        </ol>
    </div>
    
    <div class="recommendations">
        <h3>Recommendations</h3>
        <ol>
            <li><strong>Adopt Linear Regression as the baseline model</strong> for this prediction task due to its superior performance and simplicity</li>
            <li><strong>Investigate why complex models underperformed</strong> - possible issues with hyperparameters or feature engineering</li>
            <li><strong>Consider additional feature engineering</strong> to capture potential nonlinear relationships that simple models might miss</li>
            <li><strong>Implement monitoring for model drift</strong> as market conditions change over time</li>
            <li><strong>Explore ensemble approaches</strong> that combine the strengths of different model types</li>
        </ol>
    </div>
    
    <h3>Next Steps</h3>
    <p>Suggested areas for further investigation:</p>
    <ul>
        <li>Feature engineering to create additional predictive features</li>
        <li>Hyperparameter optimization for the tree-based models</li>
        <li>Testing different neural network architectures</li>
        <li>Incorporating additional data sources (news, fundamentals, etc.)</li>
        <li>Implementing a production pipeline with model monitoring</li>
    </ul>
</div>

<div class="footer">
    <p>Stock Price Prediction Analysis Report | Generated using Python Scikit-learn, XGBoost, and Matplotlib</p>
    <p>Confidential - For Internal Use Only</p>
</div>

<script>
    document.getElementById('report-date').textContent = new Date().toLocaleString();
</script>

</body>
</html>